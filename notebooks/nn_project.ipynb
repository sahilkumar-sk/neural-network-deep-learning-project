{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahilkumar-sk/neural-network-deep-learning-project/blob/main/Notebook/nn_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import timm\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import average_precision_score\n",
        "from tqdm import tqdm\n",
        "from torchvision.models import ViT_B_16_Weights, Swin_B_Weights, ResNet50_Weights"
      ],
      "metadata": {
        "collapsed": true,
        "id": "99KceFHVScXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!mkdir -p /content/VOC2008\n",
        "os.chdir('/content/VOC2008')\n",
        "\n",
        "!wget -q http://host.robots.ox.ac.uk/pascal/VOC/voc2008/VOCtrainval_14-Jul-2008.tar\n",
        "!tar -xf VOCtrainval_14-Jul-2008.tar\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "PqupVGgRlEup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "NUM_EPOCHS = 5\n",
        "LEARNING_RATE = 1e-4\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "IMG_DIR = '/content/VOC2008/VOCdevkit/VOC2008/JPEGImages'"
      ],
      "metadata": {
        "id": "u17bjsVRhbFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloaders(img_dir, batch_size=32, num_workers=0):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "    ])\n",
        "\n",
        "    train_ds = DogDataset('/content/VOC2008/VOCdevkit/VOC2008/ImageSets/Main/dog_train.txt', img_dir, transform)\n",
        "    val_ds   = DogDataset('/content/VOC2008/VOCdevkit/VOC2008/ImageSets/Main/dog_val.txt',   img_dir, transform)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "twiO2jN7YxOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DogDataset(Dataset):\n",
        "    def __init__(self, split_file, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "\n",
        "        with open(split_file, 'r') as f:\n",
        "            for line in f:\n",
        "                img_id, lbl = line.strip().split()\n",
        "                lbl = int(lbl)\n",
        "                mapped_label = 0 if lbl == -1 else 1\n",
        "                self.samples.append((img_id, mapped_label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id, lbl = self.samples[idx]\n",
        "        img_path = os.path.join(self.img_dir, f\"{img_id}.jpg\")\n",
        "\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "        except:\n",
        "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, lbl, img_id\n"
      ],
      "metadata": {
        "id": "Ig5jy1GKZuu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vit_model(num_classes=2):\n",
        "    weights = ViT_B_16_Weights.IMAGENET1K_V1\n",
        "    model = models.vit_b_16(weights=weights)\n",
        "    num_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(num_features, num_classes)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "kdjXjv6xbmji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_swin_model(num_classes=2):\n",
        "    weights = Swin_B_Weights.IMAGENET1K_V1\n",
        "    model = models.swin_b(weights=weights)\n",
        "    num_features = model.head.in_features\n",
        "    model.head = nn.Linear(num_features, num_classes)\n",
        "    return model"
      ],
      "metadata": {
        "id": "1PoYlZwNcJ71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_resnet_model(num_classes=2):\n",
        "    weights = ResNet50_Weights.IMAGENET1K_V2\n",
        "    model = models.resnet50(weights=weights)\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_features, num_classes)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "58yra5O5jvNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import densenet201, DenseNet201_Weights\n",
        "\n",
        "def create_densenet_model(num_classes=2):\n",
        "    weights = DenseNet201_Weights.IMAGENET1K_V1\n",
        "    model = densenet201(weights=weights)\n",
        "    num_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(num_features, num_classes)\n",
        "    return model"
      ],
      "metadata": {
        "id": "ahofBYwz5taE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for images, labels, _ in tqdm(loader, desc=\"Training\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "\n",
        "    return total_loss / len(loader.dataset)\n"
      ],
      "metadata": {
        "id": "yp4e8lBefOjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_map(model, loader, device):\n",
        "    model.eval()\n",
        "    all_scores, all_labels, all_ids = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels, img_ids in tqdm(loader, desc=\"Evaluating\"):\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
        "\n",
        "            all_scores.extend(probs)\n",
        "            all_labels.extend(labels.numpy())\n",
        "            all_ids.extend(img_ids)\n",
        "\n",
        "    map_score = average_precision_score(all_labels, all_scores)\n",
        "    top10 = sorted(zip(all_scores, all_ids), reverse=True)[:10]\n",
        "    return map_score, top10"
      ],
      "metadata": {
        "id": "aq4d4cVLHVDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_top10(top10, img_dir, model_name):\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    for i, (score, img_id) in enumerate(top10):\n",
        "        img_path = os.path.join(img_dir, f\"{img_id}.jpg\")\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            plt.subplot(2, 5, i+1)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"Score: {score:.4f}\")\n",
        "            plt.axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_id}: {e}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model_name}_top10_dogs.png')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KwP_0yuLH06Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model_name, model_creation_fn):\n",
        "    print(f\"Training {model_name}...\")\n",
        "\n",
        "    train_loader, val_loader = get_dataloaders(IMG_DIR, BATCH_SIZE, NUM_WORKERS)\n",
        "\n",
        "    model = model_creation_fn()\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    params = [\n",
        "        {'params': [p for n, p in model.named_parameters() if 'head' not in n and 'fc' not in n], 'lr': LEARNING_RATE/10},\n",
        "        {'params': [p for n, p in model.named_parameters() if 'head' in n or 'fc' in n], 'lr': LEARNING_RATE}\n",
        "    ]\n",
        "\n",
        "    optimizer = optim.AdamW(params, weight_decay=0.05)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "    best_map = 0.0\n",
        "    best_model_path = f\"best_{model_name}_model.pth\"\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
        "\n",
        "        val_map, top10 = evaluate_map(model, val_loader, DEVICE)\n",
        "\n",
        "        scheduler.step(val_map)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}, Validation MAP: {val_map:.4f}\")\n",
        "\n",
        "        if val_map > best_map:\n",
        "            best_map = val_map\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Saved new best model with MAP: {best_map:.4f}\")\n",
        "\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "    final_val_map, top10 = evaluate_map(model, val_loader, DEVICE)\n",
        "    print(f\"Final Validation MAP: {final_val_map:.4f}\")\n",
        "\n",
        "    print(f\"Top 10 detected dogs for {model_name}:\")\n",
        "    for i, (score, img_id) in enumerate(top10):\n",
        "        print(f\"{i+1}. Image ID: {img_id}, Confidence: {score:.4f}\")\n",
        "\n",
        "    visualize_top10(top10, IMG_DIR, model_name)\n",
        "\n",
        "    return final_val_map"
      ],
      "metadata": {
        "id": "2l13j53pkoQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_swin = train_model(\"swin\", create_swin_model)"
      ],
      "metadata": {
        "id": "X2AUDmOEwN-W",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_vit = train_model(\"vit\", create_vit_model)"
      ],
      "metadata": {
        "id": "ltsnEAofe0tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Train Swin Transformer model\n",
        "    # map_swin = train_model(\"swin\", create_swin_model)\n",
        "\n",
        "    # Train ResNet model\n",
        "    # map_resnet = train_model(\"resnet\", create_resnet_model)\n",
        "\n",
        "    # Compare results of all models (if all have been trained)\n",
        "    # print(\"\\nModel Comparison:\")\n",
        "    # print(f\"ViT MAP: {map_vit:.4f}\")\n",
        "    # print(f\"Swin MAP: {map_swin:.4f}\")\n",
        "    # print(f\"ResNet MAP: {map_resnet:.4f})"
      ],
      "metadata": {
        "id": "qOePbgXJrDbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_resnet = train_model(\"resnet\", create_resnet_model)\n"
      ],
      "metadata": {
        "id": "87sRJq3K2Ltc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_densenet = train_model(\"densenet\", create_densenet_model)"
      ],
      "metadata": {
        "id": "OcJvPbxA5FUU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import convnext_base, ConvNeXt_Base_Weights\n",
        "\n",
        "def create_convnext_model(num_classes=2):\n",
        "    weights = ConvNeXt_Base_Weights.IMAGENET1K_V1\n",
        "    model = convnext_base(weights=weights)\n",
        "    num_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "OS5uAsnV7bzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_convnext = train_model(\"convnext\", create_convnext_model)"
      ],
      "metadata": {
        "id": "7PVC27dQ7jTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nModel Comparison:\")\n",
        "print(f\"ViT MAP: {map_vit:.4f}\")\n",
        "print(f\"Swin MAP: {map_swin:.4f}\")\n",
        "# print(f\"ResNet MAP: {map_resnet:.4f}\")\n",
        "# print(f\"DenseNet MAP: {map_densenet:.4f}\")\n",
        "print(f\"ConvNeXt MAP: {map_convnext:.4f}\")\n"
      ],
      "metadata": {
        "id": "Zm1iC6we4vmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DINNING TABLE CLASSIFICATION"
      ],
      "metadata": {
        "id": "tFPL1XhHGX_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DinningTableDataset(Dataset):\n",
        "    def __init__(self, split_file, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "\n",
        "        with open(split_file, 'r') as f:\n",
        "            for line in f:\n",
        "                img_id, lbl = line.strip().split()\n",
        "                lbl = int(lbl)\n",
        "                mapped_label = 0 if lbl == -1 else 1\n",
        "                self.samples.append((img_id, mapped_label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id, lbl = self.samples[idx]\n",
        "        img_path = os.path.join(self.img_dir, f\"{img_id}.jpg\")\n",
        "\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "        except:\n",
        "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, lbl, img_id\n"
      ],
      "metadata": {
        "id": "5XPX-vAkGbBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloaders_dinning(img_dir, batch_size=32, num_workers=0):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "    ])\n",
        "\n",
        "    train_dt = DinningTableDataset('/content/VOC2008/VOCdevkit/VOC2008/ImageSets/Main/diningtable_train.txt', img_dir, transform)\n",
        "    val_dt   = DinningTableDataset('/content/VOC2008/VOCdevkit/VOC2008/ImageSets/Main/diningtable_val.txt',   img_dir, transform)\n",
        "\n",
        "    train_loader_dt = DataLoader(train_dt, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    val_loader_dt   = DataLoader(val_dt,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    return train_loader_dt, val_loader_dt"
      ],
      "metadata": {
        "id": "4W1Ih8lPGguu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_dt(model_name, model_creation_fn):\n",
        "    print(f\"Training {model_name}...\")\n",
        "\n",
        "    train_loader_dt, val_loader_dt = get_dataloaders_dinning(IMG_DIR, BATCH_SIZE, NUM_WORKERS)\n",
        "\n",
        "    model = model_creation_fn()\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    params = [\n",
        "        {'params': [p for n, p in model.named_parameters() if 'head' not in n and 'fc' not in n], 'lr': LEARNING_RATE/10},\n",
        "        {'params': [p for n, p in model.named_parameters() if 'head' in n or 'fc' in n], 'lr': LEARNING_RATE}\n",
        "    ]\n",
        "\n",
        "    optimizer = optim.AdamW(params, weight_decay=0.05)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "    best_map = 0.0\n",
        "    best_model_path = f\"best_{model_name}_model.pth\"\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_loss = train_one_epoch(model, train_loader_dt, criterion, optimizer, DEVICE)\n",
        "\n",
        "        val_map, top10 = evaluate_map(model, val_loader_dt, DEVICE)\n",
        "\n",
        "        scheduler.step(val_map)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}, Validation MAP: {val_map:.4f}\")\n",
        "\n",
        "        if val_map > best_map:\n",
        "            best_map = val_map\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Saved new best model with MAP: {best_map:.4f}\")\n",
        "\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "    final_val_map, top10 = evaluate_map(model, val_loader_dt, DEVICE)\n",
        "    print(f\"Final Validation MAP: {final_val_map:.4f}\")\n",
        "\n",
        "    print(f\"Top 10 detected Dinning Table for {model_name}:\")\n",
        "    for i, (score, img_id) in enumerate(top10):\n",
        "        print(f\"{i+1}. Image ID: {img_id}, Confidence: {score:.4f}\")\n",
        "\n",
        "    visualize_top10_dt(top10, IMG_DIR, model_name)\n",
        "\n",
        "    return final_val_map"
      ],
      "metadata": {
        "id": "INyKoXLqH70t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_top10_dt(top10, img_dir, model_name):\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    for i, (score, img_id) in enumerate(top10):\n",
        "        img_path = os.path.join(img_dir, f\"{img_id}.jpg\")\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            plt.subplot(2, 5, i+1)\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"Score: {score:.4f}\")\n",
        "            plt.axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_id}: {e}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model_name}_top10_dinning.png')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dlzxeppWJESD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload the ZIP file"
      ],
      "metadata": {
        "id": "PHC_8Q6OGiiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import io\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print(f'Unzipping file: {fn}')\n",
        "    with zipfile.ZipFile(io.BytesIO(uploaded[fn]), 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/images')  # Extract files to this folder\n",
        "\n",
        "print(\"Files extracted to /content/images\")\n"
      ],
      "metadata": {
        "id": "bMOPTH1EG5_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Images"
      ],
      "metadata": {
        "id": "4ABxVTY64vUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "test_img_paths = glob.glob('/content/images/Images/*.jpg')  # Note the extra 'Images' folder\n",
        "print(f\"Found {len(test_img_paths)} test images\")\n"
      ],
      "metadata": {
        "id": "0SBdahkBJtLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, average_precision_score\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "# ====== SETTINGS ======\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "IMG_DIR = '/content/VOC2008/VOCdevkit/VOC2008/JPEGImages'\n",
        "\n",
        "# --- Dining Table Dataset Class ---\n",
        "class DinningTableDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, split_file, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        with open(split_file, 'r') as f:\n",
        "            for line in f:\n",
        "                img_id, lbl = line.strip().split()\n",
        "                lbl = int(lbl)\n",
        "                mapped_label = 0 if lbl == -1 else 1\n",
        "                self.samples.append((img_id, mapped_label))\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        img_id, lbl = self.samples[idx]\n",
        "        img_path = f\"{self.img_dir}/{img_id}.jpg\"\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "        except:\n",
        "            img = Image.new('RGB', (224,224), (0,0,0))\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, lbl, img_id\n",
        "\n",
        "# --- Transforms ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_ds = DinningTableDataset('/content/VOC2008/VOCdevkit/VOC2008/ImageSets/Main/diningtable_val.txt', IMG_DIR, transform)\n",
        "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# ====== LOAD MODELS ======\n",
        "def create_vit_model(num_classes=2):\n",
        "    model = models.vit_b_16(weights=None)\n",
        "    model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def create_swin_model(num_classes=2):\n",
        "    model = models.swin_b(weights=None)\n",
        "    model.head = nn.Linear(model.head.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "vit_path = '/content/VOC2008/best_vit_dt_model.pth'\n",
        "swin_path = '/content/VOC2008/best_swin_dt_model.pth'\n",
        "\n",
        "vit = create_vit_model().to(DEVICE)\n",
        "swin = create_swin_model().to(DEVICE)\n",
        "\n",
        "vit.load_state_dict(torch.load(vit_path, map_location=DEVICE))\n",
        "swin.load_state_dict(torch.load(swin_path, map_location=DEVICE))\n",
        "\n",
        "vit.eval(); swin.eval()\n",
        "\n",
        "# ====== FEATURE EXTRACTORS ======\n",
        "def get_feature_extractor(model, arch):\n",
        "    def extractor(x):\n",
        "        with torch.no_grad():\n",
        "            if arch == 'vit':\n",
        "                x = model._process_input(x)\n",
        "                n = x.shape[0]\n",
        "                batch_class_token = model.class_token.expand(n, -1, -1)\n",
        "                x = torch.cat([batch_class_token, x], dim=1)\n",
        "                x = model.encoder(x)\n",
        "                feats = x[:, 0]\n",
        "            elif arch == 'swin':\n",
        "                x = model.features(x)\n",
        "                x = model.norm(x)\n",
        "                feats = x.mean([-2,-1])\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported architecture for feature extraction: {arch}\")\n",
        "        return feats\n",
        "    return extractor\n",
        "\n",
        "models_dict = {'vit': vit, 'swin': swin}\n",
        "extractors_dict = {\n",
        "    'vit': get_feature_extractor(vit, 'vit'),\n",
        "    'swin': get_feature_extractor(swin, 'swin')\n",
        "}\n",
        "\n",
        "# ====== GET PROBS & FEATURES ======\n",
        "def get_outputs_and_features(models_dict, extractors_dict, loader, device):\n",
        "    all_probs = {k: [] for k in models_dict}\n",
        "    all_feats = {k: [] for k in models_dict}\n",
        "    labels = []\n",
        "    img_ids = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls, ids in tqdm(loader, desc=\"Fusion Extraction\"):\n",
        "            imgs = imgs.to(device)\n",
        "            labels.extend(lbls.numpy())\n",
        "            img_ids.extend(ids)\n",
        "            for name in models_dict:\n",
        "                model = models_dict[name]\n",
        "                extractor = extractors_dict[name]\n",
        "                feats = extractor(imgs)\n",
        "                feats = feats.flatten(1)\n",
        "                all_feats[name].append(feats.cpu().numpy())\n",
        "                logits = model(imgs)\n",
        "                prob = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "                all_probs[name].append(prob)\n",
        "    feats_stacked = {k: np.vstack(all_feats[k]) for k in all_feats}\n",
        "    probs_stacked = {k: np.hstack(all_probs[k]) for k in all_probs}\n",
        "    return probs_stacked, feats_stacked, np.array(labels), img_ids\n",
        "\n",
        "# ====== EXTRACT FEATURES & PROBS ======\n",
        "dt_probs, dt_feats, dt_labels, _ = get_outputs_and_features(models_dict, extractors_dict, val_loader, DEVICE)\n",
        "\n",
        "# ====== LATE FUSION (Dining Table) ======\n",
        "late_probs = np.mean(np.vstack(list(dt_probs.values())), axis=0)\n",
        "late_preds = (late_probs >= 0.5).astype(int)\n",
        "print(\"\\nLate Fusion (Average):\")\n",
        "print(\"Accuracy:\", accuracy_score(dt_labels, late_preds))\n",
        "print(\"mAP:\", average_precision_score(dt_labels, late_probs))\n",
        "\n",
        "indiv_preds = [ (p >= 0.5).astype(int) for p in dt_probs.values() ]\n",
        "num_models = len(models_dict)\n",
        "majority_threshold = (num_models // 2) + (num_models % 2 > 0)\n",
        "maj_vote = (np.sum(np.vstack(indiv_preds), axis=0) >= majority_threshold).astype(int)\n",
        "print(\"\\nLate Fusion (Majority Vote):\")\n",
        "print(\"Accuracy:\", accuracy_score(dt_labels, maj_vote))\n",
        "print(\"mAP:\", average_precision_score(dt_labels, late_probs))\n",
        "\n",
        "# ====== EARLY FUSION (Dining Table) ======\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X = np.concatenate([dt_feats[k] for k in ['vit','swin']], axis=1)\n",
        "y = dt_labels\n",
        "\n",
        "scaler = StandardScaler().fit(X)\n",
        "X_scaled = scaler.transform(X)\n",
        "clf = LogisticRegression(max_iter=1000).fit(X_scaled, y)\n",
        "early_probs = clf.predict_proba(X_scaled)[:,1]\n",
        "early_preds = (early_probs >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\nEarly Fusion (LogReg):\")\n",
        "print(\"Accuracy:\", accuracy_score(y, early_preds))\n",
        "print(\"mAP:\", average_precision_score(y, early_probs))\n"
      ],
      "metadata": {
        "id": "AF9ZAkfLFNsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_vit_dt = train_model_dt(\"vit_dt\", create_vit_model)"
      ],
      "metadata": {
        "id": "Kz-xJU4QJhZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_swin_dt = train_model_dt(\"swin_dt\", create_swin_model)"
      ],
      "metadata": {
        "id": "98I6ALN5ONdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Early and Late Fusion on DOG"
      ],
      "metadata": {
        "id": "AWRDaNGAHJv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, average_precision_score\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "# ====== SETTINGS ======\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "IMG_DIR = '/content/VOC2008/VOCdevkit/VOC2008/JPEGImages'\n",
        "\n",
        "# --- Dog Dataset Class ---\n",
        "class DogDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, split_file, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        with open(split_file, 'r') as f:\n",
        "            for line in f:\n",
        "                img_id, lbl = line.strip().split()\n",
        "                lbl = int(lbl)\n",
        "                mapped_label = 0 if lbl == -1 else 1\n",
        "                self.samples.append((img_id, mapped_label))\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        img_id, lbl = self.samples[idx]\n",
        "        img_path = f\"{self.img_dir}/{img_id}.jpg\"\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "        except:\n",
        "            img = Image.new('RGB', (224,224), (0,0,0))\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, lbl, img_id\n",
        "\n",
        "# --- Transforms ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_ds = DogDataset('/content/VOC2008/VOCdevkit/VOC2008/ImageSets/Main/dog_val.txt', IMG_DIR, transform)\n",
        "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# ====== LOAD MODELS ======\n",
        "def create_vit_model(num_classes=2):\n",
        "    model = models.vit_b_16(weights=None)\n",
        "    model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def create_swin_model(num_classes=2):\n",
        "    model = models.swin_b(weights=None)\n",
        "    model.head = nn.Linear(model.head.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def create_convnext_model(num_classes=2):\n",
        "    model = models.convnext_base(weights=None)\n",
        "    # Access the original in_features before replacing the layer\n",
        "    # Assuming the structure is features -> avgpool -> classifier (norm -> flatten -> linear)\n",
        "    original_in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(original_in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "vit_path = '/content/VOC2008/best_vit_model.pth'\n",
        "swin_path = '/content/VOC2008/best_swin_model.pth'\n",
        "convnext_path = '/content/VOC2008/best_convnext_model.pth'\n",
        "\n",
        "vit = create_vit_model().to(DEVICE)\n",
        "swin = create_swin_model().to(DEVICE)\n",
        "convnext = create_convnext_model().to(DEVICE)\n",
        "\n",
        "vit.load_state_dict(torch.load(vit_path, map_location=DEVICE))\n",
        "swin.load_state_dict(torch.load(swin_path, map_location=DEVICE))\n",
        "convnext.load_state_dict(torch.load(convnext_path, map_location=DEVICE))\n",
        "\n",
        "vit.eval(); swin.eval(); convnext.eval()\n",
        "\n",
        "# ====== FEATURE EXTRACTORS ======\n",
        "def get_feature_extractor(model, arch):\n",
        "    \"\"\"\n",
        "    Returns a function that extracts features before the classification head\n",
        "    for different model architectures from torchvision.\n",
        "    \"\"\"\n",
        "    def extractor(x):\n",
        "        with torch.no_grad():\n",
        "            if arch == 'vit':\n",
        "                # For VisionTransformer, features before the head are the output\n",
        "                # from the encoder (usually the classification token).\n",
        "                # Need to call the forward pass and access the output before 'heads'.\n",
        "                # This is a simplified approach; a forward hook might be more robust\n",
        "                # if the internal structure changes.\n",
        "                # Let's manually pass through necessary parts of the ViT forward pass\n",
        "                # excluding the final head.\n",
        "                x = model._process_input(x)\n",
        "                n = x.shape[0]\n",
        "                # Expand the class token to the full batch\n",
        "                batch_class_token = model.class_token.expand(n, -1, -1)\n",
        "                x = torch.cat([batch_class_token, x], dim=1)\n",
        "                x = model.encoder(x)\n",
        "                # Features are typically the output for the class token (index 0)\n",
        "                feats = x[:, 0]\n",
        "            elif arch == 'swin':\n",
        "                # For SwinTransformer, features are obtained by passing through\n",
        "                # the features and norm layers, then global pooling.\n",
        "                x = model.features(x)\n",
        "                x = model.norm(x)\n",
        "                # Apply global average pooling manually\n",
        "                feats = x.mean([-2,-1]) # Mean across height and width dimensions\n",
        "            elif arch == 'convnext':\n",
        "                 # For ConvNeXt, pass through features and avgpool, then flatten\n",
        "                 # The classifier is usually a Sequential with Norm, Flatten, and Linear\n",
        "                 x = model.features(x)\n",
        "                 # Access avgpool directly from the model\n",
        "                 if hasattr(model, 'avgpool'):\n",
        "                     x = model.avgpool(x)\n",
        "                 # Features before the final linear layer are after the avgpool and norm/flatten\n",
        "                 # The classifier in torchvision ConvNeXt is Sequential(Norm, Flatten, Linear)\n",
        "                 # We want the output before the final Linear layer (index 2)\n",
        "                 feats = model.classifier[:2](x) # Pass through Norm and Flatten\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported architecture for feature extraction: {arch}\")\n",
        "        return feats\n",
        "    return extractor\n",
        "\n",
        "\n",
        "models_dict = {'vit': vit, 'swin': swin, 'convnext': convnext}\n",
        "extractors_dict = {\n",
        "    'vit': get_feature_extractor(vit, 'vit'),\n",
        "    'swin': get_feature_extractor(swin, 'swin'),\n",
        "    'convnext': get_feature_extractor(convnext, 'convnext')\n",
        "}\n",
        "\n",
        "# ====== GET PROBS & FEATURES ======\n",
        "def get_outputs_and_features(models_dict, extractors_dict, loader, device):\n",
        "    all_probs = {k: [] for k in models_dict}\n",
        "    all_feats = {k: [] for k in models_dict}\n",
        "    labels = []\n",
        "    img_ids = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls, ids in tqdm(loader, desc=\"Fusion Extraction\"):\n",
        "            imgs = imgs.to(device)\n",
        "            labels.extend(lbls.numpy())\n",
        "            img_ids.extend(ids)\n",
        "            for name in models_dict:\n",
        "                model = models_dict[name]\n",
        "                # Use the specific extractor for this architecture\n",
        "                extractor = extractors_dict[name]\n",
        "                feats = extractor(imgs)\n",
        "                # Ensure features are flat for concatenation later\n",
        "                feats = feats.flatten(1) # Flatten from batch_size x num_features\n",
        "                all_feats[name].append(feats.cpu().numpy())\n",
        "                logits = model(imgs) # Get logits for probability calculation\n",
        "                prob = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "                all_probs[name].append(prob)\n",
        "    feats_stacked = {k: np.vstack(all_feats[k]) for k in all_feats}\n",
        "    probs_stacked = {k: np.hstack(all_probs[k]) for k in all_probs}\n",
        "    return probs_stacked, feats_stacked, np.array(labels), img_ids\n",
        "\n",
        "# ====== EXTRACT FEATURES & PROBS ======\n",
        "dog_probs, dog_feats, dog_labels, _ = get_outputs_and_features(models_dict, extractors_dict, val_loader, DEVICE)\n",
        "\n",
        "# ====== LATE FUSION (Dog) ======\n",
        "# 1. Average Softmax\n",
        "late_probs = np.mean(np.vstack(list(dog_probs.values())), axis=0)\n",
        "late_preds = (late_probs >= 0.5).astype(int)\n",
        "print(\"\\nLate Fusion (Average):\")\n",
        "print(\"Accuracy:\", accuracy_score(dog_labels, late_preds))\n",
        "print(\"mAP:\", average_precision_score(dog_labels, late_probs))\n",
        "\n",
        "# 2. Majority Vote\n",
        "indiv_preds = [ (p >= 0.5).astype(int) for p in dog_probs.values() ]\n",
        "# Ensure correct majority vote logic - at least ceil(num_models / 2)\n",
        "num_models = len(models_dict)\n",
        "majority_threshold = (num_models // 2) + (num_models % 2 > 0) # Simple ceil(n/2)\n",
        "maj_vote = (np.sum(np.vstack(indiv_preds), axis=0) >= majority_threshold).astype(int)\n",
        "print(\"\\nLate Fusion (Majority Vote):\")\n",
        "print(\"Accuracy:\", accuracy_score(dog_labels, maj_vote))\n",
        "# Note: mAP for majority vote is usually calculated on the predicted class\n",
        "# (0 or 1), not probabilities. Using average probs mAP for consistency.\n",
        "print(\"mAP:\", average_precision_score(dog_labels, late_probs)) # Using average probs for mAP\n",
        "\n",
        "# ====== EARLY FUSION (Dog) ======\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X = np.concatenate([dog_feats[k] for k in ['vit','swin','convnext']], axis=1)\n",
        "y = dog_labels\n",
        "\n",
        "scaler = StandardScaler().fit(X)\n",
        "X_scaled = scaler.transform(X)\n",
        "clf = LogisticRegression(max_iter=1000).fit(X_scaled, y)\n",
        "early_probs = clf.predict_proba(X_scaled)[:,1]\n",
        "early_preds = (early_probs >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\nEarly Fusion (LogReg):\")\n",
        "print(\"Accuracy:\", accuracy_score(y, early_preds))\n",
        "print(\"mAP:\", average_precision_score(y, early_probs))"
      ],
      "metadata": {
        "id": "nClz6wbU8DqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pedicting Dog Images"
      ],
      "metadata": {
        "id": "4NBuxa5k4ldF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import shutil\n",
        "from google.colab import files\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Update these paths and variables as per your setup\n",
        "TEST_FOLDER = '/content/images/Images'\n",
        "\n",
        "def predict_early_fusion(img_path, extractors_dict, scaler, clf, device):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img_tensor = test_transform(img).unsqueeze(0).to(device)\n",
        "    feats_concat = []\n",
        "    for name, extractor in extractors_dict.items():\n",
        "        feats = extractor(img_tensor)\n",
        "        feats = feats.flatten(1).cpu().numpy()\n",
        "        feats_concat.append(feats)\n",
        "    feats_stack = np.concatenate(feats_concat, axis=1)\n",
        "    feats_stack = scaler.transform(feats_stack)\n",
        "    prob = clf.predict_proba(feats_stack)[:,1][0]  # Probability of 'dog' class\n",
        "    return prob\n",
        "\n",
        "# Collect test images\n",
        "test_img_paths = [os.path.join(TEST_FOLDER, f) for f in os.listdir(TEST_FOLDER)\n",
        "                  if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.webp'))]\n",
        "\n",
        "# Predict dog probabilities\n",
        "results = []\n",
        "for img_path in test_img_paths:\n",
        "    prob = predict_early_fusion(img_path, extractors_dict, scaler, clf, DEVICE)\n",
        "    results.append((img_path, prob))\n",
        "\n",
        "# Sort descending\n",
        "results_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\n=== Top 5 Images Predicted as 'Dog' (Early Fusion) ===\\n\")\n",
        "for rank, (img_path, prob) in enumerate(results_sorted[:5], start=1):\n",
        "    print(f\"Rank {rank}: {os.path.basename(img_path)} | Probability: {prob:.4f}\")\n",
        "\n",
        "# Plot top 5 images with scores in a grid\n",
        "top_results = results_sorted[:5]\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20,5))\n",
        "for ax, (img_path, score) in zip(axes, top_results):\n",
        "    img = Image.open(img_path)\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(f\"Score: {score:.4f}\")\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save top 5 images to folder and zip for download\n",
        "output_dir = '/content/top5_dog_images'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for rank, (img_path, prob) in enumerate(results_sorted[:5], start=1):\n",
        "    dest_path = os.path.join(output_dir, f'rank{rank}_{os.path.basename(img_path)}')\n",
        "    shutil.copy(img_path, dest_path)\n",
        "\n",
        "shutil.make_archive('/content/top5_dog_images_zip', 'zip', output_dir)\n",
        "files.download('/content/top5_dog_images_zip.zip')\n"
      ],
      "metadata": {
        "id": "2jmgW1hdXiFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Early and Late Fusion on Dining Table"
      ],
      "metadata": {
        "id": "e2zwpdvsNg2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, average_precision_score\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "# ====== SETTINGS ======\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "IMG_DIR = '/content/VOC2008/VOCdevkit/VOC2008/JPEGImages'\n",
        "\n",
        "# --- Dining Table Dataset Class ---\n",
        "class DinningTableDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, split_file, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        with open(split_file, 'r') as f:\n",
        "            for line in f:\n",
        "                img_id, lbl = line.strip().split()\n",
        "                lbl = int(lbl)\n",
        "                mapped_label = 0 if lbl == -1 else 1\n",
        "                self.samples.append((img_id, mapped_label))\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        img_id, lbl = self.samples[idx]\n",
        "        img_path = f\"{self.img_dir}/{img_id}.jpg\"\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "        except:\n",
        "            img = Image.new('RGB', (224,224), (0,0,0))\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, lbl, img_id\n",
        "\n",
        "# --- Transforms ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_ds = DinningTableDataset('/content/VOC2008/VOCdevkit/VOC2008/ImageSets/Main/diningtable_val.txt', IMG_DIR, transform)\n",
        "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# ====== LOAD MODELS ======\n",
        "def create_vit_model(num_classes=2):\n",
        "    model = models.vit_b_16(weights=None)\n",
        "    model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "def create_swin_model(num_classes=2):\n",
        "    model = models.swin_b(weights=None)\n",
        "    model.head = nn.Linear(model.head.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "vit_path = '/content/VOC2008/best_vit_dt_model.pth'\n",
        "swin_path = '/content/VOC2008/best_swin_dt_model.pth'\n",
        "\n",
        "vit = create_vit_model().to(DEVICE)\n",
        "swin = create_swin_model().to(DEVICE)\n",
        "\n",
        "vit.load_state_dict(torch.load(vit_path, map_location=DEVICE))\n",
        "swin.load_state_dict(torch.load(swin_path, map_location=DEVICE))\n",
        "\n",
        "vit.eval(); swin.eval()\n",
        "\n",
        "# ====== FEATURE EXTRACTORS ======\n",
        "def get_feature_extractor(model, arch):\n",
        "    def extractor(x):\n",
        "        with torch.no_grad():\n",
        "            if arch == 'vit':\n",
        "                x = model._process_input(x)\n",
        "                n = x.shape[0]\n",
        "                batch_class_token = model.class_token.expand(n, -1, -1)\n",
        "                x = torch.cat([batch_class_token, x], dim=1)\n",
        "                x = model.encoder(x)\n",
        "                feats = x[:, 0]\n",
        "            elif arch == 'swin':\n",
        "                x = model.features(x)\n",
        "                x = model.norm(x)\n",
        "                feats = x.mean([-2,-1])\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported architecture for feature extraction: {arch}\")\n",
        "        return feats\n",
        "    return extractor\n",
        "\n",
        "models_dict = {'vit': vit, 'swin': swin}\n",
        "extractors_dict = {\n",
        "    'vit': get_feature_extractor(vit, 'vit'),\n",
        "    'swin': get_feature_extractor(swin, 'swin')\n",
        "}\n",
        "\n",
        "# ====== GET PROBS & FEATURES ======\n",
        "def get_outputs_and_features(models_dict, extractors_dict, loader, device):\n",
        "    all_probs = {k: [] for k in models_dict}\n",
        "    all_feats = {k: [] for k in models_dict}\n",
        "    labels = []\n",
        "    img_ids = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls, ids in tqdm(loader, desc=\"Fusion Extraction\"):\n",
        "            imgs = imgs.to(device)\n",
        "            labels.extend(lbls.numpy())\n",
        "            img_ids.extend(ids)\n",
        "            for name in models_dict:\n",
        "                model = models_dict[name]\n",
        "                extractor = extractors_dict[name]\n",
        "                feats = extractor(imgs)\n",
        "                feats = feats.flatten(1)\n",
        "                all_feats[name].append(feats.cpu().numpy())\n",
        "                logits = model(imgs)\n",
        "                prob = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "                all_probs[name].append(prob)\n",
        "    feats_stacked = {k: np.vstack(all_feats[k]) for k in all_feats}\n",
        "    probs_stacked = {k: np.hstack(all_probs[k]) for k in all_probs}\n",
        "    return probs_stacked, feats_stacked, np.array(labels), img_ids\n",
        "\n",
        "# ====== EXTRACT FEATURES & PROBS ======\n",
        "dt_probs, dt_feats, dt_labels, _ = get_outputs_and_features(models_dict, extractors_dict, val_loader, DEVICE)\n",
        "\n",
        "# ====== LATE FUSION (Dining Table) ======\n",
        "late_probs = np.mean(np.vstack(list(dt_probs.values())), axis=0)\n",
        "late_preds = (late_probs >= 0.5).astype(int)\n",
        "print(\"\\nLate Fusion (Average):\")\n",
        "print(\"Accuracy:\", accuracy_score(dt_labels, late_preds))\n",
        "print(\"mAP:\", average_precision_score(dt_labels, late_probs))\n",
        "\n",
        "indiv_preds = [ (p >= 0.5).astype(int) for p in dt_probs.values() ]\n",
        "num_models = len(models_dict)\n",
        "majority_threshold = (num_models // 2) + (num_models % 2 > 0)\n",
        "maj_vote = (np.sum(np.vstack(indiv_preds), axis=0) >= majority_threshold).astype(int)\n",
        "print(\"\\nLate Fusion (Majority Vote):\")\n",
        "print(\"Accuracy:\", accuracy_score(dt_labels, maj_vote))\n",
        "print(\"mAP:\", average_precision_score(dt_labels, late_probs))\n",
        "\n",
        "# ====== EARLY FUSION (Dining Table) ======\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X = np.concatenate([dt_feats[k] for k in ['vit','swin']], axis=1)\n",
        "y = dt_labels\n",
        "\n",
        "scaler = StandardScaler().fit(X)\n",
        "X_scaled = scaler.transform(X)\n",
        "clf = LogisticRegression(max_iter=1000).fit(X_scaled, y)\n",
        "early_probs = clf.predict_proba(X_scaled)[:,1]\n",
        "early_preds = (early_probs >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\nEarly Fusion (LogReg):\")\n",
        "print(\"Accuracy:\", accuracy_score(y, early_preds))\n",
        "print(\"mAP:\", average_precision_score(y, early_probs))\n"
      ],
      "metadata": {
        "id": "AIWEpcQnNph6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pedicting Dining Table Images"
      ],
      "metadata": {
        "id": "tUTt7ah2N8-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming these are already defined and loaded somewhere in your notebook:\n",
        "# DEVICE, models_dict, extractors_dict, scaler, clf\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "TEST_FOLDER = '/content/images/Images'\n",
        "\n",
        "# Collect all image paths\n",
        "test_img_paths = []\n",
        "for fname in os.listdir(TEST_FOLDER):\n",
        "    if fname.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.webp')):\n",
        "        test_img_paths.append(os.path.join(TEST_FOLDER, fname))\n",
        "\n",
        "def predict_early_fusion(img_path, models_dict, extractors_dict, scaler, clf):\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img_tensor = test_transform(img).unsqueeze(0).to(DEVICE)\n",
        "    feats_concat = []\n",
        "    for name in models_dict:\n",
        "        extractor = extractors_dict[name]\n",
        "        feats = extractor(img_tensor)\n",
        "        feats = feats.flatten(1).cpu().numpy()\n",
        "        feats_concat.append(feats)\n",
        "    feats_stack = np.concatenate(feats_concat, axis=1)\n",
        "    feats_stack = scaler.transform(feats_stack)\n",
        "    early_prob = clf.predict_proba(feats_stack)[:,1][0]\n",
        "    return early_prob\n",
        "\n",
        "# Get prediction probabilities for all test images\n",
        "results = []\n",
        "for img_path in test_img_paths:\n",
        "    early_prob = predict_early_fusion(img_path, models_dict, extractors_dict, scaler, clf)\n",
        "    results.append((img_path, early_prob))\n",
        "\n",
        "# Sort by probability descending\n",
        "results_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\n=== Top 5 Images Predicted as 'Dining Table' (Early Fusion) ===\\n\")\n",
        "for i, (img_path, early_prob) in enumerate(results_sorted[:5]):\n",
        "    img_name = os.path.basename(img_path)\n",
        "    print(f\"Rank {i+1}: {img_name} | Early Fusion Probability: {early_prob:.3f}\")\n",
        "\n",
        "# Plot top 5 images with scores in a grid\n",
        "top_results = results_sorted[:5]\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20,5))\n",
        "for ax, (img_path, score) in zip(axes, top_results):\n",
        "    img = Image.open(img_path)\n",
        "    ax.imshow(img)\n",
        "    ax.set_title(f\"Score: {score:.4f}\")\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save top 5 images to folder and zip for download\n",
        "output_dir = '/content/top5_dining_table_images'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for rank, (img_path, prob) in enumerate(results_sorted[:5], start=1):\n",
        "    dest_path = os.path.join(output_dir, f'rank{rank}_{os.path.basename(img_path)}')\n",
        "    shutil.copy(img_path, dest_path)\n",
        "\n",
        "shutil.make_archive('/content/top5_dining_table_images_zip', 'zip', output_dir)\n",
        "files.download('/content/top5_dining_table_images_zip.zip')\n"
      ],
      "metadata": {
        "id": "P1q69n6jIo_u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
